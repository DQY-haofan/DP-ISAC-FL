# æ–‡ä»¶å: generate_viz_data_ultra.py
# ä½œç”¨: ç”Ÿæˆ IEEE é¡¶åˆŠæ‰€éœ€çš„æ·±åº¦æœºç†æ•°æ® (CSV + NPY)
# æ ¸å¿ƒ: é€šè¿‡ç±»ç»§æ‰¿ï¼Œ100% å¤ç°åŸå§‹ stga.py å’Œ aggregators.py çš„é€»è¾‘ï¼Œç¡®ä¿æ•°æ®çœŸå®ã€‚

import torch
import numpy as np
import os
import yaml
import pandas as pd
import shutil
import torch.nn.functional as F
from tqdm import tqdm
import copy

# å¼•å…¥åŸå§‹å·¥ç¨‹æ–‡ä»¶
from server import Server
from stga import STGAAggregator
from aggregators import KrumAggregator, FedAvgAggregator
from datasets import partition_dataset_dirichlet, get_dataset
from secure_isac import SecureISACScheduler


# ==============================================================================
# 1. æ’æ¡©ç»„ä»¶ (Instrumented Components)
#    è¿™äº›ç±»ç»§æ‰¿è‡ªåŸå§‹ä»£ç ï¼Œä¿ç•™åŸæ±åŸå‘³çš„é€»è¾‘ï¼Œä»…æ·»åŠ æ•°æ®æ•è·åŠŸèƒ½ã€‚
# ==============================================================================

class InstrumentedSTGA(STGAAggregator):
    """
    ç»§æ‰¿ STGAAggregatorï¼Œå®Œæ•´ä¿ç•™ stga.py çš„é€»è¾‘ï¼ˆNorm Clipping, Spatial, Temporalï¼‰ã€‚
    é¢å¤–åŠŸèƒ½ï¼šæ•è·ä¸­é—´å˜é‡ (weights, norms, cosines)ã€‚
    """

    def __init__(self, config):
        super().__init__(config)
        self.captured_data = None

    def aggregate(self, updates, client_types=None):
        if not updates: return None

        # --- [REPLICATING ORIGINAL LOGIC START] ---
        # ä¸ºäº†ç¡®ä¿æ•°æ®å®Œå…¨ä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œè·å–ä¸­é—´å˜é‡ã€‚
        # ç”±äºåŸå§‹ aggregate æ–¹æ³•ä¸è¿”å›æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦é‡å†™å®ƒï¼Œä½†ä¿æŒé€»è¾‘å®Œå…¨ç›¸åŒã€‚

        flat_updates = [self._flatten(u) for u in updates]
        update_matrix = torch.stack(flat_updates).to(self.device)

        # 1. Norm Clipping (ä¸ stga.py ä¸€è‡´)
        update_norms = torch.norm(update_matrix, p=2, dim=1)
        median_norm = torch.median(update_norms)
        threshold = median_norm * 1.5
        clip_factor = torch.clamp(threshold / (update_norms + 1e-6), max=1.0)
        update_matrix_clipped = update_matrix * clip_factor.unsqueeze(1)

        # 2. Spatial Consistency (ä¸ stga.py ä¸€è‡´)
        spatial_center = torch.median(update_matrix_clipped, dim=0).values
        s_spat_cos = F.cosine_similarity(update_matrix_clipped, spatial_center.unsqueeze(0), dim=1)

        # è·ç¦»åˆ†
        dists = torch.norm(update_matrix_clipped - spatial_center, p=2, dim=1)
        sigma = torch.median(dists) + 1e-6
        s_spat_dist = torch.exp(-dists / sigma)
        s_spat = (s_spat_cos + 1) / 2 * 0.5 + s_spat_dist * 0.5

        # 3. Temporal Consistency (ä¸ stga.py ä¸€è‡´)
        if len(self.history_updates) > 0:
            expected_update = self.history_updates[-1].to(self.device)
            s_temp = F.cosine_similarity(update_matrix_clipped, expected_update.unsqueeze(0), dim=1)
        else:
            s_temp = torch.ones(len(updates)).to(self.device)

        # 4. Trust Score & Softmax (ä¸ stga.py ä¸€è‡´)
        s_temp_norm = (s_temp + 1) / 2
        trust_scores = self.alpha * s_temp_norm + (1 - self.alpha) * s_spat
        weights = F.softmax(trust_scores * 2.0, dim=0)  # Softmax temperature = 2.0

        # 5. èšåˆ
        weighted_update_vec = torch.mv(update_matrix_clipped.t(), weights)
        self.history_updates.append(weighted_update_vec.detach().cpu())
        # --- [REPLICATING ORIGINAL LOGIC END] ---

        # [CAPTURE] æ•è·å…³é”®æœºç†æ•°æ®
        self.captured_data = {
            'norms': update_norms.detach().cpu().numpy(),  # åŸå§‹æ¨¡é•¿
            'cosines': s_spat_cos.detach().cpu().numpy(),  # æ–¹å‘ä¸€è‡´æ€§
            'weights': weights.detach().cpu().numpy(),  # æœ€ç»ˆæƒé‡
            'updates': update_matrix.detach().cpu().numpy()  # åŸå§‹é«˜ç»´å‘é‡ (ç”¨äº t-SNE)
        }

        return self._unflatten(weighted_update_vec, updates[0])


class InstrumentedKrum(KrumAggregator):
    """ ç»§æ‰¿ KrumAggregatorï¼Œæ•è·è¢«é€‰ä¸­èŠ‚ç‚¹çš„ç´¢å¼•ä½œä¸ºæƒé‡ã€‚ """

    def __init__(self, f_malicious=2):
        super().__init__(f_malicious)
        self.captured_data = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

    def aggregate(self, updates, **kwargs):
        if not updates: return None
        n = len(updates)

        # å‡†å¤‡æ•°æ®ç”¨äºè®¡ç®—æŒ‡æ ‡ (Norms, Cosines)
        flat_list = [
            torch.cat([v.view(-1) for k, v in sorted(u.items()) if v.dtype == torch.float32])
            for u in updates
        ]
        stack = torch.stack(flat_list).to(self.device)

        norms = torch.norm(stack, p=2, dim=1).detach().cpu().numpy()
        center = torch.median(stack, dim=0).values
        cosines = F.cosine_similarity(stack, center.unsqueeze(0), dim=1).detach().cpu().numpy()

        # --- Krum Logic (å¤ç° aggregators.py) ---
        dists = torch.cdist(stack, stack)
        k_neighbors = n - self.f - 2
        if k_neighbors < 1: k_neighbors = 1
        scores = []
        for i in range(n):
            d_sorted, _ = torch.sort(dists[i])
            scores.append(torch.sum(d_sorted[1: 1 + k_neighbors]))
        scores = torch.tensor(scores)

        m = max(1, n - self.f)
        top_k_indices = torch.topk(scores, m, largest=False).indices

        # [CAPTURE]
        weights = np.zeros(n)
        weights[top_k_indices.cpu().numpy()] = 1.0 / m  # Krum æ˜¯ç¡¬é€‰æ‹©ï¼Œé€‰ä¸­å³å‡åˆ†

        self.captured_data = {
            'norms': norms,
            'cosines': cosines,
            'weights': weights,
            'updates': stack.detach().cpu().numpy()
        }

        # è°ƒç”¨çˆ¶ç±»å®Œæˆå®é™…èšåˆ (KrumAggregator.aggregate å·²ç»å®ç°äº† Multi-Krum)
        return super().aggregate(updates, **kwargs)


class InstrumentedFedAvg(FedAvgAggregator):
    """ ç»§æ‰¿ FedAvgAggregatorï¼Œè®°å½•å‡åŒ€æƒé‡ã€‚ """

    def __init__(self):
        super().__init__()
        self.captured_data = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

    def aggregate(self, updates, **kwargs):
        n = len(updates)

        flat_list = [
            torch.cat([v.view(-1) for k, v in sorted(u.items()) if v.dtype == torch.float32])
            for u in updates
        ]
        stack = torch.stack(flat_list).to(self.device)
        norms = torch.norm(stack, p=2, dim=1).detach().cpu().numpy()
        center = torch.median(stack, dim=0).values
        cosines = F.cosine_similarity(stack, center.unsqueeze(0), dim=1).detach().cpu().numpy()

        self.captured_data = {
            'norms': norms,
            'cosines': cosines,
            'weights': np.full(n, 1.0 / n),
            'updates': stack.detach().cpu().numpy()
        }
        return super().aggregate(updates, **kwargs)


class InstrumentedServer(Server):
    """ ç»§æ‰¿ Serverï¼Œç”¨äºæ•è· ISAC Mask å’Œè°ƒåº¦è¿‡ç¨‹ã€‚ """

    def __init__(self, config, train_dataset, client_indices):
        super().__init__(config, train_dataset, client_indices)
        self.captured_mask = None

    def run_round(self, round_idx):
        # è°ƒç”¨çˆ¶ç±» run_round
        stats = super().run_round(round_idx)

        # [CAPTURE] æ•è· ISAC æ©ç 
        # Server ç±»ä¸­ self.isac_scheduler.last_mask å­˜å‚¨äº†æœ€æ–°çš„æ©ç 
        if self.isac_scheduler.last_mask is not None:
            self.captured_mask = self.isac_scheduler.last_mask.cpu().numpy()

        return stats


# ==============================================================================
# 2. é‡‡é›†ä¸»ç¨‹åº (Harvester Main)
# ==============================================================================

def run_ultra_harvest():
    print("ğŸšœ Starting Ultra-Deep Data Harvest (Fig 1-12 Source Data)...")
    os.makedirs('viz_data', exist_ok=True)

    # è¯»å–åŸºç¡€é…ç½®
    with open('config.yaml') as f:
        base_conf = yaml.safe_load(f)

    # --- [CRITICAL CONFIG] è®¾å®šé«˜å‹å´©æºƒç¯å¢ƒ (Fig 11/12 çš„å…³é”®) ---
    base_conf['attack']['malicious_fraction'] = 0.3  # 30% æ¶æ„èŠ‚ç‚¹ (Krum å´©æºƒç‚¹)
    base_conf['attack']['lambda_attack'] = 5.0  # å¼ºæ”»å‡» (æ¨¡é•¿æ”¾å¤§æ˜¾è‘—)
    base_conf['num_rounds'] = 30  # è·‘ 30 è½®çœ‹ç¨³æ€

    scenarios = ['FedAvg', 'Krum', 'R-JORA']
    all_records = []

    for mode in scenarios:
        print(f"\nğŸ“¡ Harvesting Scenario: {mode} ...")
        conf = copy.deepcopy(base_conf)
        conf['scenario'] = mode

        # é…ç½®èšåˆå™¨ä¸é˜²å¾¡å¼€å…³
        if mode == 'R-JORA':
            conf['aggregator'] = 'STGA'
            conf['r_jora']['enabled'] = True
        elif mode == 'Krum':
            conf['aggregator'] = 'Krum'
            conf['r_jora']['enabled'] = False  # å…³é—­å…¶ä»–é˜²å¾¡ï¼Œå•æµ‹èšåˆå™¨
        else:
            conf['aggregator'] = 'FedAvg'
            conf['r_jora']['enabled'] = False

        # åˆå§‹åŒ–
        ds, _ = get_dataset(conf['dataset'], conf['data_root'])
        # å›ºå®š Seed 42 ä¿è¯æ•°æ®åˆ†å¸ƒ (Non-IID) ä¸€è‡´æ€§
        idx = partition_dataset_dirichlet(ds, conf['num_clients'], conf['alpha'], seed=42)

        # ä½¿ç”¨æ’æ¡© Server
        server = InstrumentedServer(conf, ds, idx)

        # æ›¿æ¢ä¸ºæ’æ¡©èšåˆå™¨
        if mode == 'R-JORA':
            server.aggregator = InstrumentedSTGA(conf)
        elif mode == 'Krum':
            f_mal = int(conf['num_clients'] * conf['client_fraction'] * 0.3) + 2
            server.aggregator = InstrumentedKrum(f_malicious=f_mal)
        else:
            server.aggregator = InstrumentedFedAvg()

        # è¿è¡Œ
        for t in tqdm(range(conf['num_rounds'])):
            server.run_round(t)

            # 1. æå–èšåˆå™¨å†…éƒ¨æ•°æ®
            agg_data = server.aggregator.captured_data
            if agg_data is None: continue

            norms = agg_data['norms']
            cosines = agg_data['cosines']
            weights = agg_data['weights']
            updates = agg_data['updates']

            # 2. åˆ¤å®šèŠ‚ç‚¹èº«ä»½ (åŸºäºæ¨¡é•¿èšç±»ï¼Œå› ä¸º lambda=5.0 å¯¼è‡´æ¶æ„æ¨¡é•¿æ˜¾è‘—)
            # è¿™æ˜¯ä¸€ä¸ªå‡†ç¡®çš„åéªŒæ ‡è®°æ–¹æ³•
            median_norm = np.median(norms)
            types = []
            for n in norms:
                if n > median_norm * 2.0:  # æ¶æ„èŠ‚ç‚¹æ¨¡é•¿é€šå¸¸ > 5.0 * median
                    types.append('Malicious')
                else:
                    types.append('Benign')

            # 3. è®°å½•åˆ° DataFrame List
            for i in range(len(norms)):
                all_records.append({
                    'Scenario': mode,
                    'Round': t,
                    'Type': types[i],
                    'L2_Norm': norms[i],
                    'Cosine_Sim': cosines[i],
                    'Weight': weights[i]
                })

            # 4. ä¿å­˜ .npy æ–‡ä»¶ (ç”¨äº t-SNE, Heatmap, Mask)
            # ä»…ä¿å­˜ R-JORA çš„å…³é”®å¸§å’Œ Maskï¼Œå‡å°‘å­˜å‚¨å‹åŠ›
            if mode == 'R-JORA':
                # ä¿å­˜ Mask ç”¨äº Fig 8
                if server.captured_mask is not None:
                    np.save(f'viz_data/mask_r{t}.npy', server.captured_mask)

                # ä¿å­˜ Updates ç”¨äº t-SNE (Fig 6) - é€‰å‡ ä¸ªå…³é”®è½®æ¬¡
                if t in [0, 5, 15, 29]:
                    np.save(f'viz_data/updates_r{t}.npy', updates)
                    np.save(f'viz_data/types_r{t}.npy', np.array(types))

                # ä¿å­˜æƒé‡çŸ©é˜µç”¨äº Heatmap (Fig 7) - æ¯è½®éƒ½å­˜ï¼Œä½†åªå­˜å‰20ä¸ªå®¢æˆ·ç«¯
                # æ³¨æ„ï¼šselected_clients æ¯è½®éƒ½åœ¨å˜ï¼ŒHeatmap éœ€è¦ ID å¯¹åº”ã€‚
                # ä¸ºäº†ç®€åŒ– Heatmapï¼Œæˆ‘ä»¬åªç”» "Selected Clients" çš„æƒé‡åˆ†å¸ƒï¼Œæˆ–è€…ä¸ç”» ID è½´ã€‚
                # è¿™é‡Œä¿å­˜åŸå§‹ weights æ•°ç»„
                np.save(f'viz_data/weights_r{t}.npy', weights)

    # å¯¼å‡º CSV
    df = pd.DataFrame(all_records)
    df.to_csv('viz_metrics_pro.csv', index=False)
    print("\nâœ… Harvest Complete! Files generated:")
    print("   - viz_metrics_pro.csv (Source for Fig 9, 11, 12)")
    print("   - viz_data/*.npy (Source for Fig 6, 7, 8)")


if __name__ == "__main__":
    run_ultra_harvest()