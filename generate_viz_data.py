# ============================================================
# è„šæœ¬å: generate_viz_universal.py
# ä½œç”¨: 1. é‡‡é›†æ‰€æœ‰å¯è§†åŒ–æ•°æ® (.npy ç”¨äºç»˜å›¾)
#       2. å¯¼å‡º viz_metrics.csv (ç”¨äºäººå·¥æ£€æŸ¥å’Œè°ƒè¯•)
# ä¿®å¤: åœ¨ Clipping ä¹‹å‰æ•è· Normsï¼Œè§£å†³ Fig 11 é—®é¢˜
# ============================================================
import torch
import numpy as np
import os
import yaml
import pandas as pd
import shutil
import torch.nn.functional as F
from server import Server
from stga import STGAAggregator
from datasets import partition_dataset_dirichlet, get_dataset


# --- é—´è°èšåˆå™¨ (æ•°æ®æ¢é’ˆ) ---
class UniversalProbe(STGAAggregator):
    def __init__(self, config):
        super().__init__(config)
        # ä¸´æ—¶å­˜å‚¨æœ¬è½®æ•°æ®
        self.round_data = {
            'norms': None, 'cosines': None, 'weights': None, 'updates': None
        }

    def aggregate(self, updates, client_types=None):
        if not updates: return None

        # 1. å±•å¹³æ•°æ®
        flat_updates = [self._flatten(u) for u in updates]
        update_matrix = torch.stack(flat_updates).to(self.device)

        # [Fix for Fig 11] åœ¨ä»»ä½•é˜²å¾¡å¤„ç†ä¹‹å‰ï¼Œæ•è·åŸå§‹ç‰¹å¾ï¼
        raw_norms = torch.norm(update_matrix, p=2, dim=1)

        # è®¡ç®—ç›¸å¯¹äºåŸå§‹ä¸­å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦ (çœ‹æ”»å‡»è€…ä¼ªè£…å¾—åƒä¸åƒ)
        raw_center = torch.median(update_matrix, dim=0).values
        raw_cosines = F.cosine_similarity(update_matrix, raw_center.unsqueeze(0), dim=1)

        # å­˜å…¥ç¼“å­˜
        self.round_data['norms'] = raw_norms.detach().cpu().numpy()
        self.round_data['cosines'] = raw_cosines.detach().cpu().numpy()
        self.round_data['updates'] = update_matrix.detach().cpu().numpy()

        # --- æ‰§è¡Œæ­£å¸¸çš„ STGA é€»è¾‘ (ä»¥è®¡ç®—æƒé‡) ---
        # 1. Clipping
        median_norm = torch.median(raw_norms)
        threshold = median_norm * 1.5
        clip_factor = torch.clamp(threshold / (raw_norms + 1e-6), max=1.0)
        update_matrix_clipped = update_matrix * clip_factor.unsqueeze(1)

        # 2. Spatial Score
        spatial_center = torch.median(update_matrix_clipped, dim=0).values
        s_spat_cos = F.cosine_similarity(update_matrix_clipped, spatial_center.unsqueeze(0), dim=1)
        dists = torch.norm(update_matrix_clipped - spatial_center, p=2, dim=1)
        sigma = torch.median(dists) + 1e-6
        s_spat_dist = torch.exp(-dists / sigma)
        s_spat = (s_spat_cos + 1) / 2 * 0.5 + s_spat_dist * 0.5

        # 3. Temporal Score
        if len(self.history_updates) > 0:
            expected_update = self.history_updates[-1].to(self.device)
            s_temp = F.cosine_similarity(update_matrix_clipped, expected_update.unsqueeze(0), dim=1)
        else:
            s_temp = torch.ones(len(updates)).to(self.device)
        s_temp_norm = (s_temp + 1) / 2

        # 4. Final Weights
        trust_scores = self.alpha * s_temp_norm + (1 - self.alpha) * s_spat
        weights = F.softmax(trust_scores * 2.0, dim=0)

        # å­˜å…¥ç¼“å­˜
        self.round_data['weights'] = weights.detach().cpu().numpy()

        return super().aggregate(updates, client_types)


# --- ä¸»ç¨‹åº ---
def run_universal_harvest():
    print("ğŸš€ Starting Universal Data Harvest...")

    # 1. ç¯å¢ƒæ¸…ç†ä¸é…ç½®
    if os.path.exists('viz_data'): shutil.rmtree('viz_data')
    os.makedirs('viz_data', exist_ok=True)

    with open('config.yaml') as f:
        conf = yaml.safe_load(f)

    # å¼ºåˆ¶é…ç½®: R-JORA + å¼ºæ”»å‡» (ä¸ºäº†è®©ç‰¹å¾æ˜æ˜¾)
    if 'r_jora' not in conf: conf['r_jora'] = {}
    conf['r_jora'].update({'enabled': True, 'enable_stga': True, 'enable_optimal_dp': True, 'enable_secure_isac': True})
    if 'stga_alpha' not in conf['r_jora']: conf['r_jora']['stga_alpha'] = 0.5

    conf['num_rounds'] = 20
    conf['scenario'] = 'Viz_Harvest'
    conf['aggregator'] = 'STGA'
    # ä½¿ç”¨ Lambda=5.0 ç¡®ä¿ Fig 11 ä¸­æ¶æ„èŠ‚ç‚¹ Norm é£èµ·æ¥
    conf['attack'] = {'malicious_fraction': 0.2, 'lambda_attack': 5.0, 'tau_sim': 0.5, 't_vgae': 1, 'q_eaves': 0.8,
                      'eaves_sigma': 0.005, 'vgae_epochs': 5, 'vgae_lr': 0.01, 'latent_dim': 16}

    if torch.cuda.is_available(): conf['device'] = 'cuda'

    # 2. åˆå§‹åŒ–
    ds, _ = get_dataset(conf['dataset'], conf['data_root'])
    idx = partition_dataset_dirichlet(ds, conf['num_clients'], conf['alpha'], seed=42)
    server = Server(conf, ds, idx)
    server.aggregator = UniversalProbe(conf)  # æ³¨å…¥æ¢é’ˆ

    # å…¨å±€ CSV æ•°æ®å®¹å™¨
    csv_records = []

    # 3. è¿è¡Œå¾ªç¯
    print(f"   Running {conf['num_rounds']} rounds with Lambda={conf['attack']['lambda_attack']}...")
    for t in range(conf['num_rounds']):
        server.run_round(t)

        # æå–æ¢é’ˆæ•°æ®
        data = server.aggregator.round_data
        if data['weights'] is None: continue

        # è·å–æœ¬è½®é€‰ä¸­çš„å®¢æˆ·ç«¯ ID (å‡è®¾ run_round å†…éƒ¨é¡ºåºä¸€è‡´)
        # è¿™é‡Œæˆ‘ä»¬è¦ç¨å¾® hack ä¸€ä¸‹ï¼šserver.run_round é‡Œçš„ selected_clients æ˜¯å±€éƒ¨å˜é‡ã€‚
        # ä½†æˆ‘ä»¬çŸ¥é“ updates çš„é¡ºåºå°±æ˜¯ selected_clients çš„é¡ºåºã€‚
        # ä¸”æˆ‘ä»¬çŸ¥é“ typesã€‚

        # é‡æ–°æ¨å¯¼ Client Type (é€šè¿‡æƒé‡æ¨æ–­ï¼šæƒé‡æä½çš„å¤§æ¦‚ç‡æ˜¯æ¶æ„ï¼Œä½†è¿™ä¸ä¸¥è°¨)
        # ä¸¥è°¨åšæ³•ï¼šæˆ‘ä»¬éœ€è¦ server å‘Šè¯‰æˆ‘ä»¬è¿™è½®é€‰äº†è°ã€‚
        # ç®€åŒ–åšæ³•ï¼šæˆ‘ä»¬åœ¨ CSV é‡Œåªè®°å½• 'Type' (Malicious/Benign) è€Œä¸è®°å½•å…·ä½“ IDï¼Œè¿™è¶³å¤Ÿç”»å›¾äº†ã€‚

        # è¿™é‡Œçš„ data['weights'] é•¿åº¦ä¸º K (æ¯”å¦‚10)ã€‚
        # æˆ‘ä»¬æ€ä¹ˆçŸ¥é“å“ªä¸ªæ˜¯ Maliciousï¼Ÿ
        # å›åˆ° server.py, malicious_updates æ˜¯ååŠ è¿›å»çš„ã€‚
        # é€šå¸¸ server.run_round é‡Œ: benign_updates + malicious_updates
        # æ‰€ä»¥å‰ N ä¸ªæ˜¯ Benignï¼Œå M ä¸ªæ˜¯ Maliciousã€‚

        num_mal = int(len(data['weights']) * conf['attack']['malicious_fraction'])  # 2
        num_ben = len(data['weights']) - num_mal  # 8

        # æ„é€ ç±»å‹æ ‡ç­¾åˆ—è¡¨
        current_types = ['Benign'] * num_ben + ['Malicious'] * num_mal

        # ä¿å­˜åˆ° NPY (ç”¨äº plot_ieee è„šæœ¬)
        np.save(f'viz_data/weights_r{t}.npy', data['weights'])
        np.save(f'viz_data/norms_r{t}.npy', data['norms'])
        np.save(f'viz_data/cosines_r{t}.npy', data['cosines'])
        np.save(f'viz_data/client_types_r{t}.npy', np.array(current_types))  # æ¯è½®å­˜ä¸€ä»½ç±»å‹

        # ä¿å­˜åˆ° CSV åˆ—è¡¨
        for i in range(len(data['weights'])):
            csv_records.append({
                'Round': t,
                'Client_Index_In_Batch': i,
                'Type': current_types[i],
                'L2_Norm': data['norms'][i],
                'Cosine_Sim': data['cosines'][i],
                'Weight': data['weights'][i]
            })

        # ä¿å­˜ t-SNE å‘é‡ (ä»…å…³é”®å¸§)
        if t in [0, 5, 10, 19]:
            np.save(f'viz_data/updates_r{t}.npy', data['updates'])
            # å¯¹åº”çš„ç±»å‹ä¹Ÿå­˜ä¸€ä¸‹ï¼Œæ–¹ä¾¿ t-SNE ç”»å›¾è„šæœ¬è¯»å–
            np.save(f'viz_data/client_types_tsne_r{t}.npy', np.array(current_types))

    # 4. å¯¼å‡º CSV
    df = pd.DataFrame(csv_records)
    df.to_csv('viz_metrics.csv', index=False)
    print(f"âœ… Saved 'viz_metrics.csv' ({len(df)} rows).")
    print("   Columns: Round, Type, L2_Norm, Cosine_Sim, Weight")

    # é¡ºä¾¿ç”Ÿæˆä¸€ä¸ª client_types.npy ç»™æ—§ç»˜å›¾è„šæœ¬å…¼å®¹
    # æ³¨æ„ï¼šæ—§è„šæœ¬å¯èƒ½å‡è®¾è¿™é‡Œå­˜çš„æ˜¯æ‰€æœ‰ 100 ä¸ªå®¢æˆ·ç«¯çš„ç±»å‹ã€‚
    # ä¸ºäº†å…¼å®¹ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªå…¨é‡çš„
    all_types = np.array(['Malicious' if i in server.malicious_ids else 'Benign' for i in range(conf['num_clients'])])
    np.save('viz_data/client_types.npy', all_types)


if __name__ == "__main__":
    run_universal_harvest()